# -*- coding: utf-8 -*-
"""Home Assignment 4 Solution.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z33_lS7ONkCCfJgERxLFEQ4HgdF4epb6

Home Assignment 4

Student Name: SAI SRAVAN CHINTALA

Student ID: 700773836

1. GAN Architecture

Explain the adversarial process in GAN training. What are the goals of the generator and discriminator, and how do they improve through competition? Diagram of the GAN architecture showing the data flow and objectives of each component.

Answer: Generative Adversarial Networks (GANs) are a class of neural networks where two models‚Äîthe generator and the discriminator‚Äîcompete against each other in a zero-sum game. The goal is for the generator to create data that is indistinguishable from real data, while the discriminator tries to differentiate between real and generated data.

Goals:
Generator (G):

Objective: Generate realistic data samples that can fool the discriminator.

Loss Function: Minimize the probability of the discriminator correctly identifying the generated samples as "fake."

Discriminator (D):

Objective: Accurately classify whether the input data is real or generated.

Loss Function: Maximize the probability of correctly distinguishing between real and fake data.

Training Process:
The generator starts by producing random samples from a latent space (e.g., random noise).

These samples are fed into the discriminator, which outputs a probability indicating whether each sample is real or fake.

Based on the discriminator's feedback:

The discriminator is trained to improve its classification accuracy.

The generator adjusts its parameters to create more realistic samples that better fool the discriminator.

Over time:

The generator learns to produce samples that resemble the real dataset.

The discriminator improves its ability to detect fake samples, making it increasingly difficult for the generator.

The training continues until an equilibrium is reached, where the discriminator cannot reliably distinguish between real and generated data.

Diagram of GAN Architecture:
Below is a textual description of the architecture diagram:

Input to the Generator (Latent Space): A random vector
ùëß
z (noise).

Generator Network: Processes
ùëß
z to create a synthetic data sample
ùê∫
(
ùëß
)
G(z).

Discriminator Network:

Takes two inputs:

Real data samples from the dataset.

Generated samples
ùê∫
(
ùëß
)
G(z) from the generator.

Outputs probabilities:

ùê∑
(
ùë•
)
D(x): Probability that real data
ùë•
x is real.

ùê∑
(
ùê∫
(
ùëß
)
)
D(G(z)): Probability that the generated data
ùê∫
(
ùëß
)
G(z) is fake.

Loss Feedback Loops:

Generator Loss: Based on
1
‚àí
ùê∑
(
ùê∫
(
ùëß
)
)
1‚àíD(G(z)), encourages the generator to improve.

Discriminator Loss: Based on
ùê∑
(
ùë•
)
D(x) and
ùê∑
(
ùê∫
(
ùëß
)
)
D(G(z)), ensures it gets better at classifying.

2. Ethics and AI Harm

Choose one of the following real-world AI harms discussed in Chapter 12:

‚Ä¢	Representational harm

‚Ä¢	Allocational harm

‚Ä¢	Misinformation in generative AI

Describe a real or hypothetical application where this harm may occur. Then, suggest two harm mitigation strategies that could reduce its impact based on the lecture.

Answer: Ethics and AI Harm: Misinformation in Generative AI
Real-World Example:
Consider a generative AI system capable of producing realistic but fake news articles. For instance, it generates a headline like "Renowned Scientist Predicts Imminent Global Ice Age," accompanied by fabricated content and sources. This misinformation could quickly spread through social media, leading to public panic and the dissemination of false scientific knowledge.

Harm Mitigation Strategies:
Content Verification Tools:

Integrate robust fact-checking mechanisms within the generative AI pipeline. Before finalizing content, the AI system could cross-reference generated outputs with reliable databases or trusted knowledge repositories.

User Accountability and Traceability:

Add mechanisms to embed traceable metadata in generated content, identifying the AI model, version, and generation timestamp. This can discourage malicious use and enable easier identification of fake content.

3. Programming Task (Basic GAN Implementation)

Implement a simple GAN using PyTorch or TensorFlow to generate handwritten digits from the MNIST dataset.

Requirements:

‚Ä¢	Generator and Discriminator architecture

‚Ä¢	Training loop with alternating updates

‚Ä¢	Show sample images at Epoch 0, 50, and 100

Deliverables:

‚Ä¢	Generated image samples

‚Ä¢	Screenshot or plots comparing losses of generator and discriminator over time
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, utils # Import utils here
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np # Import numpy here

class Generator(nn.Module):
    def __init__(self, latent_dim, img_dim):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, img_dim),
            nn.Tanh()
        )

    def forward(self, z):
        return self.model(z)

class Discriminator(nn.Module):
    def __init__(self, img_dim):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(img_dim, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, img):
        return self.model(img)

# Hyperparameters
latent_dim = 100
img_dim = 28 * 28
lr = 0.0002
batch_size = 64
epochs = 100

# Data loader
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
train_data = datasets.MNIST(root="./data", train=True, download=True, transform=transform)
dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)

generator = Generator(latent_dim, img_dim)
discriminator = Discriminator(img_dim)

optimizer_G = optim.Adam(generator.parameters(), lr=lr)
optimizer_D = optim.Adam(discriminator.parameters(), lr=lr)

criterion = nn.BCELoss()

device = "cuda" if torch.cuda.is_available() else "cpu"
generator.to(device)
discriminator.to(device)

g_losses, d_losses = [], [] # Initialize lists before the loop

for epoch in range(epochs):
    for i, (real_imgs, _) in enumerate(dataloader):
        real_imgs = real_imgs.view(-1, img_dim).to(device)
        batch_size = real_imgs.size(0)

        # Labels
        real_labels = torch.ones((batch_size, 1)).to(device)
        fake_labels = torch.zeros((batch_size, 1)).to(device)

        # Train Discriminator
        z = torch.randn((batch_size, latent_dim)).to(device)
        fake_imgs = generator(z)
        real_loss = criterion(discriminator(real_imgs), real_labels)
        fake_loss = criterion(discriminator(fake_imgs.detach()), fake_labels)
        d_loss = real_loss + fake_loss

        optimizer_D.zero_grad()
        d_loss.backward()
        optimizer_D.step()

        # Train Generator
        fake_imgs = generator(z)
        g_loss = criterion(discriminator(fake_imgs), real_labels)

        optimizer_G.zero_grad()
        g_loss.backward()
        optimizer_G.step()

    # Logging and sample generation
    # Append losses after each epoch's training
    g_losses.append(g_loss.item())
    d_losses.append(d_loss.item())

    if epoch % 10 == 0 or epoch in [0, 50, 100]:
        print(f"Epoch {epoch}: D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}")
        with torch.no_grad():
            z = torch.randn(16, latent_dim).to(device)
            samples = generator(z).view(-1, 1, 28, 28).cpu()
            grid = utils.make_grid(samples, nrow=4, normalize=True) # Use utils.make_grid
            plt.imshow(grid.permute(1, 2, 0).squeeze(), cmap="gray")
            plt.title(f"Epoch {epoch}")
            plt.show()

# After training:
plt.plot(np.arange(epochs), g_losses, label="Generator Loss")
plt.plot(np.arange(epochs), d_losses, label="Discriminator Loss")
plt.legend()
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("GAN Training Losses")
plt.show()

"""4. Programming Task (Data Poisoning Simulation)

Simulate a data poisoning attack on a sentiment classifier.
Start with a basic classifier trained on a small dataset (e.g., movie reviews).

Then, poison some training data by flipping labels for phrases about a specific entity (e.g., "UC Berkeley").

Deliverables:

‚Ä¢	Graphs showing accuracy and confusion matrix before and after poisoning

‚Ä¢	How the poisoning affected results
"""

# Ensure necessary libraries are installed
!pip install torch torchvision matplotlib numpy scikit-learn

# Import necessary modules
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np # Import numpy here

# Define the SentimentClassifier class (You may need to adjust this based on your specific model architecture)
class SentimentClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super(SentimentClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.fc = nn.Linear(embedding_dim, hidden_dim) # Example layer, adjust as needed
        self.relu = nn.ReLU()
        self.output_layer = nn.Linear(hidden_dim, 1) # Output a single value for binary classification
        self.sigmoid = nn.Sigmoid() # Use Sigmoid for binary classification output

    def forward(self, x):
        embedded = self.embedding(x)
        # Assuming input x is token IDs, average embeddings for a simple model
        pooled = torch.mean(embedded, dim=1)
        hidden = self.relu(self.fc(pooled))
        output = self.sigmoid(self.output_layer(hidden))
        return output

# --- Data Loading and Preprocessing ---
# Placeholder for loading your movie review dataset
# You need to replace this with your actual data loading and preprocessing code
# Example:
# Load data from CSV, tokenize, build vocabulary, convert to tensors
# For demonstration, let's create some dummy data:
vocab_size = 10000 # Example vocabulary size
embedding_dim = 50 # Example embedding dimension
hidden_dim = 64 # Example hidden dimension
learning_rate = 0.001 # Example learning rate
epochs = 10 # Example number of epochs

# Create dummy data (replace with your actual data loading and processing)
# This dummy data represents sequences of token IDs
X = torch.randint(0, vocab_size, (100, 50)) # 100 samples, sequence length 50
y = torch.randint(0, 2, (100,)) # 100 samples, binary labels (0 or 1)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- Data Poisoning Simulation ---
# Placeholder for your data poisoning logic
# Identify

"""5. Legal and Ethical Implications of GenAI

Discuss the legal and ethical concerns of AI-generated content based on the examples of:

‚Ä¢	Memorizing private data (e.g., names in GPT-2)

‚Ä¢	Generating copyrighted material (e.g., Harry Potter text)

Do you believe generative AI models should be restricted from certain data during training? Justify your answer.

Answer: Legal and Ethical Concerns of AI-Generated Content
1. Memorizing Private Data
Generative AI models can inadvertently memorize private or sensitive information from the training data, such as:

Personal names

Addresses

Proprietary information or confidential business data

This poses significant risks, including:

Privacy violations: Disclosure of personal data without consent breaches privacy laws, such as GDPR and CCPA.

Security threats: Leakage of sensitive data (e.g., passwords, medical information) can lead to identity theft or fraud.

2. Generating Copyrighted Material
AI models, when trained on copyrighted material, can reproduce it verbatim or generate derivative content. This raises issues like:

Intellectual property infringement: Reproducing large chunks of text (e.g., from "Harry Potter") can violate copyright laws.

Fair use ambiguity: Determining whether the use of copyrighted material in training constitutes fair use is often unclear and jurisdiction-specific.

Restrictions on Training Data
Argument for Restriction:
Preventing Data Misuse:
Training generative models on unrestricted datasets increases the likelihood of privacy breaches and legal violations. For example, including sensitive medical records or financial data could lead to inadvertent disclosure of private details.

Encouraging Responsible AI Development:
Restricting training data to publicly available, ethically sourced, and copyright-compliant datasets ensures:

Respect for privacy and intellectual property.

Promotion of trust in AI systems.

Legal Compliance:
Regulatory frameworks (e.g., GDPR, DMCA) mandate organizations to protect sensitive data. Ensuring that restricted data isn‚Äôt part of the training set helps avoid legal consequences.

Argument Against Restriction:
Limiting Model Capabilities:
Over-restricting data could stifle innovation and hinder models from achieving state-of-the-art performance. For instance, excluding valuable but sensitive datasets might reduce the diversity and richness of the training material.

Cost and Practicality:
Filtering and curating training data can be resource-intensive, especially for large-scale datasets. Smaller organizations might face challenges competing with larger firms.

Suggested Mitigation Strategies:
Differential Privacy:
Implement techniques to ensure the model doesn‚Äôt memorize specific training examples. For instance, noise can be added to the data to obscure individual contributions.

Explicit Data Licensing:
Train models only on datasets where permissions have been obtained or use cases explicitly fall under fair use or open licensing.

Data Filtering and Audits:
Conduct rigorous pre-training data audits to identify and exclude sensitive or copyrighted material.

6. Bias & Fairness Tools

Visit Aequitas Bias Audit Tool.

Choose a bias metric (e.g., false negative rate parity) and describe:

‚Ä¢	What the metric measures

‚Ä¢	Why it's important

‚Ä¢	How a model might fail this metric

Optional: Try applying the tool to any small dataset or use demo data.

Answer: Exploring Aequitas Bias Audit Tool: False Negative Rate Parity
1. What the Metric Measures:
False Negative Rate (FNR) parity evaluates whether the rate of false negatives is similar across different groups or sub-populations within a dataset.

False Negative (FN): When the model predicts a negative outcome for a sample that should have been positive (e.g., predicting "No Loan Default" for a person who actually defaults).

FNR Formula:
ùêπ
ùëÅ
ùëÖ
=
False¬†Negatives¬†(FN)/
False¬†Negatives¬†(FN)
+
True¬†Positives¬†(TP)

Parity Definition:
FNR parity ensures that the rate of missed positive predictions (false negatives) is equal for all demographic groups (e.g., gender, race, age).

2. Why It's Important:
Avoiding Disparities in Outcomes:
False negatives can disproportionately harm certain groups. For example:

In healthcare, a model predicting "No Disease" for someone who is actually sick can delay life-saving treatment.

In criminal justice, a recidivism model misclassifying high-risk individuals as low-risk might jeopardize public safety.

Ethical Decision-Making:
Ensures fairness in critical domains where consequences of false negatives can be severe, promoting equity and trust.

Legal and Regulatory Compliance:
Many regulations (e.g., EU AI Act, U.S. anti-discrimination laws) require equitable treatment across demographic groups.

3. How a Model Might Fail This Metric:
A model may exhibit a higher false negative rate for one group than another due to:

Data Imbalance:
Training data may lack sufficient representation of certain groups, leading to biased learning.

Feature Correlation with Bias:
Certain features (e.g., ZIP code or education level) may inadvertently encode biases.

Overfitting to Majority Groups:
The model might learn patterns that favor well-represented groups at the expense of minorities.
"""

# Ensure necessary libraries are installed
!pip install aequitas
import pandas as pd
from aequitas.group import Group
from aequitas.bias import Bias
from aequitas.plotting import Plot

# Example data
data = pd.DataFrame({
    "score": [1, 0, 1, 0, 1, 1, 0, 0],
    "label_value": [1, 0, 1, 1, 0, 1, 0, 0],
    "gender": ["Male", "Female", "Male", "Female", "Female", "Male", "Female", "Male"],
})

g = Group()
xtab, _ = g.get_crosstabs(data)

b = Bias()
# Pass the original data DataFrame as 'original_df'
bias_df = b.get_disparity_predefined_groups(xtab,
                                           original_df=data, # Added the missing argument
                                           ref_groups_dict={'gender': 'Male'},
                                           alpha=0.05)

# Print the columns of bias_df to find the correct column name
print("Columns in bias_df:", bias_df.columns)

# Use the correct column name for false negative rate disparity
# Based on Aequitas documentation and common naming conventions, it's likely
# 'fnr_disparity'. Let's verify with the printed columns.
# Assuming the correct column name is 'fnr_disparity' based on inspecting the output of the print statement above:
print(bias_df[['attribute_name', 'attribute_value', 'fnr_disparity']]) # Changed column name here

p = Plot()
# Added the 'attribute_name' argument as required by the method
p.plot_disparity(bias_df, 'fnr_disparity', attribute_name='gender')

# The repeated code block below is unnecessary and can be removed.
# Example data
# data = pd.DataFrame({
#     "score": [1, 0, 1, 0, 1, 1, 0, 0],
#     "label_value": [1, 0, 1, 1, 0, 1, 0, 0],
#     "gender": ["Male", "Female", "Male", "Female", "Female", "Male", "Female", "Male"],
# })
# g = Group()
# xtab, _ = g.get_crosstabs(data)
# b = Bias()
# bias_df = b.get_disparity_predefined_groups(xtab,
#                                            ref_groups_dict={'gender': 'Male'},
#                                            alpha=0.05)
# print(bias_df[['attribute_name', 'attribute_value', 'false_negative_rate_disparity']])
# from aequitas.plotting import Plot
# p = Plot()
# p.plot_disparity(bias_df, 'false_negative_rate_disparity')